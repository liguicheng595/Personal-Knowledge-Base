{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d7c704",
   "metadata": {},
   "source": [
    "# Optimizing_Save_and_Load_Model\n",
    "\n",
    "在之前的学习中，已经涵盖的内容包括：\n",
    "- Tensor：Tensor的组成，运算等\n",
    "- 自动梯度计算：这一机制是梯度下降法训练模型的基础。\n",
    "- 利用 `torch.nn` 构建模型：通过该模块的神经网络层和函数来搭建深度学习架构。\n",
    "- Dataset 和 DataLoader：了解这两个概念如何简化数据加载和预处理流程，使数据可以高效地送入模型进行训练。\n",
    "- TensorBoard 可视化：使用此工具监控和展示训练过程中的各种指标。\n",
    "\n",
    "接下来这段视频将介绍的新工具：\n",
    "\n",
    "- **损失函数详解**：探讨不同的损失函数及它们各自的适用场景。\n",
    "- **PyTorch优化器**：学习多种优化算法，这些算法用于依据损失函数的结果更新模型参数。\n",
    "- **保存和加载模型**：学习如何保存模型和加载模型\n",
    "\n",
    "最终，我们将看到：\n",
    "\n",
    "- **PyTorch 完整训练循环演示**：将上述所有概念整合，实现一个实际运行的模型训练过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad55537",
   "metadata": {},
   "source": [
    "## 一、Loss Function\n",
    "下面介绍下pytorch中常用的损失函数，通过[损失函数官方文档介绍](https://pytorch.org/docs/stable/nn.html#loss-functions)可以查看\n",
    "\n",
    "在深度学习中，损失函数（或称为目标函数）是用来衡量模型输出与目标之间差异的关键组件。PyTorch 提供了多种基础损失函数，以下是一些常用的损失函数及其简要介绍：\n",
    "\n",
    "### 1. `nn.MSELoss`\n",
    "- **全称**: Mean Squared Error Loss（均方误差损失）\n",
    "- **用途**: 回归问题\n",
    "- **公式**: \n",
    "  $\n",
    "  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\n",
    "  $\n",
    "- **示例**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efea803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4353, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "loss = loss_fn(input, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f13368",
   "metadata": {},
   "source": [
    "### 2. `nn.CrossEntropyLoss`\n",
    "- **全称**: Cross Entropy Loss（交叉熵损失）\n",
    "- **用途**: 多分类问题\n",
    "- **公式**: \n",
    "  $\n",
    "  \\text{CrossEntropy} = -\\sum_{i=1}^n y_i \\log(\\hat{y}_i)\n",
    "  $\n",
    "- **示例**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a87d20b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5621, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0474, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "print(output)\n",
    "\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201623f2",
   "metadata": {},
   "source": [
    "### 3. `nn.BCELoss`\n",
    "\n",
    "- **全称**: Binary Cross Entropy Loss（二元交叉熵损失）\n",
    "- **用途**: 二分类问题\n",
    "- **公式**: \n",
    "  $\n",
    "  \\text{BCE} = -\\frac{1}{n} \\sum_{i=1}^n [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]\n",
    "  $\n",
    "- **示例**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a2b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "input = torch.sigmoid(torch.randn(3, requires_grad=True))\n",
    "target = torch.empty(3).random_(2)  # 二元标签\n",
    "loss = loss_fn(input, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d1fd7b",
   "metadata": {},
   "source": [
    "### 4. `nn.BCEWithLogitsLoss`\n",
    "- **全称**: Binary Cross Entropy Loss with Logits（二元交叉熵损失带Logits）\n",
    "- **用途**: 二分类问题，结合了`Sigmoid`和`BCELoss`\n",
    "- **公式**: 与BCELoss相同，但输入不需要先通过`Sigmoid`\n",
    "- **示例**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46662ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)  # 二元标签\n",
    "loss = loss_fn(input, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f368c",
   "metadata": {},
   "source": [
    "\n",
    "### 5. `nn.NLLLoss`\n",
    "- **全称**: Negative Log Likelihood Loss（负对数似然损失）\n",
    "- **用途**: 多分类问题，与`LogSoftmax`配合使用\n",
    "- **公式**: \n",
    "  $\n",
    "  \\text{NLL} = -\\sum_{i=1}^n \\log(\\hat{y}_{i, y_i})\n",
    "  $\n",
    "- **示例**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a31a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5719, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "input = torch.nn.functional.log_softmax(input, dim=1)\n",
    "target = torch.tensor([1, 0, 4])  # 类别标签\n",
    "loss = loss_fn(input, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d92a45",
   "metadata": {},
   "source": [
    "### 6. `nn.L1Loss`\n",
    "- **全称**: Mean Absolute Error Loss（平均绝对误差损失）\n",
    "- **用途**: 回归问题\n",
    "- **公式**: \n",
    "  $\n",
    "  \\text{L1} = \\frac{1}{n} \\sum_{i=1}^n |\\hat{y}_i - y_i|\n",
    "  $\n",
    "- **示例**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36e9d194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0285, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "loss = loss_fn(input, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455049fd",
   "metadata": {},
   "source": [
    "\n",
    "### 7. `nn.SmoothL1Loss`\n",
    "- **全称**: Smooth L1 Loss（平滑L1损失）\n",
    "- **用途**: 回归问题，结合了`L1`和`L2`损失的优点\n",
    "- **公式**: 类似于`L1Loss`，在误差较小时使用`L2`，误差较大时使用`L1`\n",
    "- **示例**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1c7168e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5512, grad_fn=<SmoothL1LossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.SmoothL1Loss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "loss = loss_fn(input, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59db1f",
   "metadata": {},
   "source": [
    "\n",
    "### 8. `nn.KLDivLoss`\n",
    "- **全称**: Kullback-Leibler Divergence Loss（KL散度损失）\n",
    "- **用途**: 衡量两个概率分布的差异，通常用于生成模型\n",
    "- **公式**: \n",
    "  $\n",
    "  \\text{KL} = \\sum_{i=1}^n y_i (\\log(y_i) - \\log(\\hat{y}_i))\n",
    "  $\n",
    "- **示例**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ede4b5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2344, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\anaconda3\\envs\\jupyter\\lib\\site-packages\\torch\\nn\\functional.py:2976: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.KLDivLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "input = torch.nn.functional.log_softmax(input, dim=1)\n",
    "target = torch.randn(3, 5)\n",
    "target = torch.nn.functional.softmax(target, dim=1)\n",
    "loss = loss_fn(input, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824dbd30",
   "metadata": {},
   "source": [
    "下面是一个关于交叉熵损失和KL散度的一些理解：\n",
    "\n",
    "[为什么交叉熵（cross-entropy）可以用于计算代价？](https://www.zhihu.com/question/65288314/answer/244557337)\n",
    "\n",
    "[交叉熵损失函数（Cross Entropy Loss）](https://zhuanlan.zhihu.com/p/638725320)\n",
    "\n",
    "\n",
    "**注意：**\n",
    "\n",
    "实际上，损失函数是一个模型中最核心的部分，一般来说是根据我们自己的任务自行设计损失函数，不会用官方提供的。损失函数设计也是最难的一部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ffcdf",
   "metadata": {},
   "source": [
    "## 二、Optimizer\n",
    "\n",
    "Optimizer的主要职责是更新模型的参数（权重和偏置）以最小化损失函数。具体来说，优化器会使用梯度（通过反向传播计算得到）来调整参数。不同的优化算法有不同的更新规则和参数调整策略，例如 SGD（随机梯度下降）、Adam、RMSprop 等。\n",
    "\n",
    "下面介绍下有关`torch.optim.Optimizer`相关的知识和pytorch中常用的优化器，通过[优化器官方文档介绍](https://pytorch.org/docs/stable/optim.html)可以查看。\n",
    "\n",
    "### 2.1 torch.optim.Optimizer\n",
    "`CLASS torch.optim.Optimizer(params, defaults)`\n",
    "\n",
    "`torch.optim.Optimizer` 是 PyTorch 提供的优化器的基类。它包含多个参数和功能函数，用于管理和更新模型参数。以下是一些关键的参数和功能函数：\n",
    "\n",
    "**1.关键参数**\n",
    "- params：需要优化的参数（通常是模型的参数）\n",
    "- defaults： 包含优化器默认配置的字典，如学习率（lr）、动量（momentum）等,也就是params后的所有参数会打包成**defaults字典**。如`SGD`优化器的`defaults`参数：\n",
    "```python\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr=0.01, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False):\n",
    "        # 验证参数\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        \n",
    "        # 将所有参数存储在 defaults 中\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov)\n",
    "        super(SGD, self).__init__(params, defaults)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbf0479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.01, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.01, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义一个简单的线性模型\n",
    "model = nn.Linear(10, 1)\n",
    "\n",
    "\n",
    "# 创建一个 SGD 优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.01)\n",
    "\n",
    "# 打印优化器的 defaults\n",
    "print(optimizer.defaults)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144dc990",
   "metadata": {},
   "source": [
    "**2.主要功能函数**\n",
    "- `__init__(self, params, defaults)`: 初始化优化器。params 是需要优化的参数，defaults 是包含默认配置的字典。\n",
    "+ `zero_grad(self, set_to_none=False)`: 将所有优化的参数的梯度清零。通常在每次反向传播之前调用，以避免累积梯度。set_to_none：如果为 True，则将梯度设置为 None，而不是零。这样做可以稍微提高性能。\n",
    "- `step(self, closure=None)`: 执行单步优化（参数更新）。这是用户在每个训练步骤中调用的主要函数。`closure`：一个可调用的函数，用于重新计算模型并返回损失。在某些优化算法（如 L-BFGS）中是必需的。\n",
    "+ `add_param_group(self, param_group)`: 向优化器添加参数组。参数组是一组具有相同优化设置的参数。`param_group`：一个字典，包含参数和它们的优化设置。\n",
    "```python\n",
    "param_group = {'params': model.layer.parameters(), 'lr': 1e-3}\n",
    "optimizer.add_param_group(param_group)\n",
    "```\n",
    "一个优化器可以包含多个参数组`param_group`的，**后面添加的参数组如果未指定参数优化设置（如lr），则默认使用`defaults`的参数，而且两个参数组中的参数是不能相同的**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91498a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': [Parameter containing:\n",
      "tensor([[ 0.1144, -0.0594,  0.0309,  0.0003, -0.1368,  0.2984, -0.0255,  0.3095,\n",
      "          0.0515,  0.1672]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1795], requires_grad=True)], 'lr': 0.01, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.01, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}\n",
      "{'params': [Parameter containing:\n",
      "tensor([[ 0.1362,  0.2245, -0.1155, -0.0211,  0.0911,  0.2797, -0.0533, -0.1589,\n",
      "          0.1296,  0.0926]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0492], requires_grad=True)], 'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.01, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}\n"
     ]
    }
   ],
   "source": [
    "model1 = nn.Linear(5, 1)\n",
    "# 定义一个新的参数组\n",
    "new_param_group = {'params': model1.parameters(), 'lr': 0.001}\n",
    "\n",
    "# 添加新的参数组到优化器中\n",
    "optimizer.add_param_group(new_param_group)\n",
    "\n",
    "# 打印所有参数组\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(param_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4119598",
   "metadata": {},
   "source": [
    "- `state_dict(self)`: 返回优化器的状态字典。**通常用于保存优化器状态，以便后续恢复训练。**\n",
    "```python\n",
    "torch.save(optimizer.state_dict(), 'optimizer.pth')\n",
    "```\n",
    "+ `load_state_dict(self, state_dict)`:**加载优化器的状态字典。通常用于恢复训练。**\n",
    "```python\n",
    "optimizer.load_state_dict(torch.load('optimizer.pth'))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0f6a8b",
   "metadata": {},
   "source": [
    "### 2.2 常见的优化器\n",
    "\n",
    "**1.SGD (随机梯度下降, Stochastic Gradient Descent)**\n",
    "\n",
    "`torch.optim.SGD(params, lr=0.001, momentum=0, dampening=0, weight_decay=0)`，列举出常用的参数：\n",
    "- `params`: 要优化的参数，可以是一个iterable或者是定义了参数组的字典。**一般是model.parameters()，返回一个生成器。**\n",
    "- `lr `(学习率): 控制每次参数更新的步长。默认值为0.001。\n",
    "- `momentum`: 动量因子，有助于加速收敛和减少振荡。默认值为0。**动量因子在梯度下降中引入了物理中的动量概念，通过考虑之前梯度的方向和大小来加速收敛并减少振荡。**\n",
    "- `dampening`: 动量的抑制因子。默认值为0。\n",
    "- `weight_decay`: 权重衰减（L2惩罚）。默认值为0。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236edf4",
   "metadata": {},
   "source": [
    "**2. Adam(Adaptive Moment Estimation)**\n",
    "\n",
    "`torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0)`，列举出常用的参数：\n",
    "- `params`: 要优化的参数，可以是一个iterable或者是定义了参数组的字典。\n",
    "- `lr `(学习率): 控制每次参数更新的步长。默认值为0.001。\n",
    "- `betas`: 用于计算一阶和二阶矩估计的系数。默认值为(0.9, 0.999)。\n",
    "- `eps (epsilon)`: 防止除零错误的小数。默认值为1e-8。\n",
    "- `weight_decay`: 权重衰减（L2惩罚）。默认值为0。 \n",
    "\n",
    "Adam（Adaptive Moment Estimation）优化算法结合了动量（Momentum）和RMSProp的优点，通过自适应调整每个参数的学习率来进行优化。以下是Adam的更新公式：\n",
    "\n",
    "$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $\n",
    "\n",
    "$ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 $\n",
    "\n",
    "$ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} $\n",
    "\n",
    "$ \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} $\n",
    "\n",
    "$ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} $\n",
    "\n",
    "其中：\n",
    "- $ m_t $ 是一阶矩估计（动量），对应于梯度的指数加权移动平均。\n",
    "- $ v_t $ 是二阶矩估计（方差），对应于梯度平方的指数加权移动平均。\n",
    "- $ \\beta_1 $ 和 $ \\beta_2 $ 分别控制一阶和二阶矩估计的指数衰减率。\n",
    "- $ g_t $ 是当前梯度。\n",
    "- $ \\eta $ 是学习率。\n",
    "- $ \\epsilon $ 是一个小数，用于防止除零错误。\n",
    "\n",
    "**`betas` 参数的意义**\n",
    "\n",
    "- **`beta1` (默认值0.9)**:\n",
    "  - 控制一阶矩估计（动量项） \\( m_t \\) 的指数衰减率。\n",
    "  - 较大的 `beta1` 值（接近1）意味着动量项更依赖于过去的梯度信息，平滑效果更明显，但反应速度稍慢。\n",
    "  - 较小的 `beta1` 值意味着动量项更依赖于当前梯度，反应更迅速，但平滑效果减弱。\n",
    "  - 通常设为0.9，可以在梯度更新中获得较好的平滑效果，避免震荡。\n",
    "\n",
    "- **`beta2` (默认值0.999)**:\n",
    "  - 控制二阶矩估计（方差项） \\( v_t \\) 的指数衰减率。\n",
    "  - 较大的 `beta2` 值（接近1）意味着方差估计更依赖于过去的梯度平方信息，平滑效果更明显。\n",
    "  - 较小的 `beta2` 值意味着方差估计更依赖于当前的梯度平方，反应更迅速，但平滑效果减弱。\n",
    "  - 通常设为0.999，能够更稳定地调整学习率，因为梯度平方的变化通常较慢。\n",
    "\n",
    "更多关于优化器算法的知识请看(https://arxiv.org/pdf/1609.04747.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f624f17",
   "metadata": {},
   "source": [
    "### 2.3 动态调整学习率（torch.optim.lr_scheduler）\n",
    "\n",
    "PyTorch提供了一些工具来根据训练的进度（例如训练的轮数）动态调整学习率。具体地，`torch.optim.lr_scheduler` 模块中包含了几种不同的方法，其中 `torch.optim.lr_scheduler.ReduceLROnPlateau` 可以根据验证集的表现动态调整学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b33fee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9708653688430786, Learning Rate: 0.001\n",
      "Epoch 2, Loss: 0.9710237383842468, Learning Rate: 0.0001\n",
      "Epoch 3, Loss: 0.9710696339607239, Learning Rate: 1e-05\n",
      "Epoch 4, Loss: 0.9710732102394104, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 5, Loss: 0.9710735082626343, Learning Rate: 1.0000000000000002e-07\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单的模型\n",
    "model = nn.Linear(10, 1)\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 定义学习率调度器\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# 或者使用 ReduceLROnPlateau 调度器\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "# 假设我们有一个数据加载器，提供批量数据\n",
    "data_loader = [(torch.randn(32, 10), torch.randn(32, 1)) for _ in range(100)]  # 示例数据\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(5):\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        optimizer.zero_grad()   # 清除梯度\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        loss.backward()  # 反向传播计算梯度\n",
    "        optimizer.step()  # 更新参数\n",
    "\n",
    "    # 在每个epoch结束时更新学习率\n",
    "    scheduler.step()\n",
    "\n",
    "    # 如果使用 ReduceLROnPlateau 调度器\n",
    "    # scheduler.step(loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Learning Rate: {optimizer.param_groups[0][\"lr\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a9a1c",
   "metadata": {},
   "source": [
    "**1. StepLR**\n",
    "\n",
    "**场景**：\n",
    "适用于当训练过程中的每隔多少阶段学习率需要按固定比例降低时。\n",
    "\n",
    "**适用情况**：\n",
    "通常在固定的epoch间隔后学习率需要调整，如常见的初始快速下降，然后逐渐减缓的训练策略。\n",
    "\n",
    "**参数**：\n",
    "- `optimizer`: 优化器实例。\n",
    "- `step_size`: 每隔多少个epoch调整一次学习率。\n",
    "- `gamma`: 学习率的衰减因子。\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e79679",
   "metadata": {},
   "source": [
    "**2. MultiStepLR**\n",
    "\n",
    "**场景**：\n",
    "适用于需要在特定的多个时间点调整学习率的场景。\n",
    "\n",
    "**适用情况**：\n",
    "通常用于已经确定好需要在哪些epoch进行学习率调整的训练策略。例如，当模型在某些训练阶段的学习速度减缓时，你可以选择在这些时刻降低学习率，以帮助模型在后期更细致地调整参数。\n",
    "\n",
    "**参数**：\n",
    "- `optimizer`: 优化器实例。\n",
    "- `milestones`: 一个包含若干epoch值的列表，在这些epoch进行学习率调整。\n",
    "- `gamma`: 学习率的衰减因子。\n",
    "\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e20d74f",
   "metadata": {},
   "source": [
    "**3. ExponentialLR**\n",
    "\n",
    "**场景**：\n",
    "适用于学习率需要持续以指数方式衰减的场景。\n",
    "\n",
    "**适用情况**：\n",
    "常用于训练中希望学习率以平滑的方式逐渐减小的情况，以提高模型的训练稳定性。\n",
    "\n",
    "**参数**：\n",
    "- `optimizer`: 优化器实例。\n",
    "- `gamma`: 每个epoch学习率的衰减因子。\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a71adb",
   "metadata": {},
   "source": [
    "**4. ReduceLROnPlateau**\n",
    "\n",
    "**场景**：\n",
    "适用于需要根据验证集性能动态调整学习率的场景。\n",
    "\n",
    "**适用情况**：\n",
    "常用于监控验证损失，当验证损失不再下降时减小学习率，以避免过拟合并改善模型的训练效果。\n",
    "\n",
    "**参数**：\n",
    "- `optimizer`: 优化器实例。\n",
    "- `mode`: 'min' 或 'max'。根据验证集指标的最小值（'min'）或最大值（'max'）来调整学习率。\n",
    "- `factor`: 学习率的衰减因子。学习率将乘以 `factor`。\n",
    "- `patience`: 在验证集指标不再提升时，等待多少个epoch再调整学习率。\n",
    "- `verbose`: 是否输出学习率调整的信息（默认为 `False`）。\n",
    "\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# 定义模型\n",
    "model = ...  # 替换为你的模型\n",
    "criterion = torch.nn.CrossEntropyLoss()  # 替换为你的损失函数\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 定义 ReduceLROnPlateau 调度器\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "# 假设我们有一个数据加载器，提供批量数据\n",
    "data_loader = [(torch.randn(32, 10), torch.randint(0, 2, (32,))) for _ in range(100)]  # 示例数据\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 假设计算验证损失\n",
    "    model.eval()\n",
    "    val_loss = calculate_validation_loss()  # 需要实现的验证损失计算函数\n",
    "\n",
    "    # 更新学习率\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss}, Learning Rate: {optimizer.param_groups[0][\"lr\"]}')\n",
    "\n",
    "def calculate_validation_loss():\n",
    "    # 实现你的验证损失计算逻辑\n",
    "    return torch.tensor(0.1)  # 示例返回值\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cea29",
   "metadata": {},
   "source": [
    "5.CosineAnnealingLR\n",
    "\n",
    "场景：\n",
    "适用于需要在训练过程中周期性地调整学习率的场景。\n",
    "\n",
    "适用情况：\n",
    "适用于训练周期固定、希望学习率在每个周期内从大到小再回升的情况。`CosineAnnealingLR` 调度器可以帮助模型在训练中保持多样化的学习率变化，通常与重启策略（如SGDR）结合使用，以更好地探索参数空间和提高训练效果。\n",
    "\n",
    "参数：\n",
    "- `optimizer`: 优化器实例。\n",
    "- `T_max`: 学习率退火周期的最大值。表示学习率将在 `T_max` 个epoch内从初始值衰减到最小值，然后再回升。\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "```\n",
    "\n",
    "6.CyclicLR\n",
    "\n",
    "场景：\n",
    "适用于需要在两个边界之间周期性地变化学习率的场景。\n",
    "\n",
    "适用情况：\n",
    "适用于希望在训练过程中学习率在预定范围内不断变化的情况。`CyclicLR` 调度器通过在训练期间周期性地调整学习率，能够增强模型的泛化能力并加速收敛。\n",
    "\n",
    "参数：\n",
    "- `optimizer`: 优化器实例。\n",
    "- `base_lr`: 最小学习率，即周期中的低点。\n",
    "- `max_lr`: 最大学习率，即周期中的高点。\n",
    "- `step_size_up`: 增加学习率的步长，即从 `base_lr` 到 `max_lr` 的周期长度。\n",
    "- `step_size_down` (可选): 减少学习率的步长，即从 `max_lr` 回到 `base_lr` 的周期长度。如果未指定，则默认为 `step_size_up` 的值。\n",
    "- `mode` (可选): 周期变化模式，默认为 'triangular'，还可以选择 'triangular2' 和 'exp_range'。\n",
    "- `cycle_momentum` (可选): 是否在周期中调整动量（默认为 `False`）。\n",
    "- `base_momentum` (可选): 基本动量值。\n",
    "- `max_momentum` (可选): 最大动量值。\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n",
    "data_loader = torch.utils.data.DataLoader(...)\n",
    "for epoch in range(10):\n",
    "    for batch in data_loader:\n",
    "        train_batch(...)\n",
    "        scheduler.step()\n",
    "```\n",
    "7.OneCycleLR\n",
    "\n",
    "场景：\n",
    "适用于一个训练周期内需要先增加学习率再减少学习率的情况，通常用于提升模型性能和训练效率。\n",
    "\n",
    "适用情况：\n",
    "`OneCycleLR` 是一种周期性学习率调度策略，适用于希望在一个训练周期内快速找到合适的学习率并进行训练的情况。该调度器通过在训练的前半部分逐步增加学习率，后半部分逐步减少学习率，帮助模型达到更好的训练效果和收敛速度。\n",
    "\n",
    "参数：\n",
    "- `optimizer`: 优化器实例。\n",
    "- `max_lr`: 一个周期内的最大学习率。\n",
    "- `total_steps` 或 `steps_per_epoch` 和 `epochs`:\n",
    "  - `total_steps`: 总训练步数。使用此参数时，`steps_per_epoch` 和 `epochs` 参数将被忽略。\n",
    "  - `steps_per_epoch`: 每个epoch中的训练步数。\n",
    "  - `epochs`: 总训练周期数。\n",
    "- `pct_start` (可选): 学习率增加到 `max_lr` 的周期比例，默认值为0.3。\n",
    "- `anneal_strategy` (可选): 学习率衰减策略，默认为 'cos'，也可以设置为 'linear'。\n",
    "- `div_factor` (可选): 最大学习率与最小学习率的比例因子，默认为 25.0。\n",
    "- `final_div_factor` (可选): 最终学习率与最大学习率的比例因子，默认为 1e4。\n",
    "- `cycle_momentum` (可选): 是否在周期中调整动量（默认为 `True`）。\n",
    "- `base_momentum` (可选): 基本动量值（默认为 0.85）。\n",
    "- `max_momentum` (可选): 最大动量值（默认为 0.95）。\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=100, epochs=10)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for batch_inputs, batch_targets in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52860730",
   "metadata": {},
   "source": [
    "### 2.4 Pytorch_Warmup（结合torch.optim.lr_scheduler一起使用）\n",
    "\n",
    "[pytorch_warmup](https://github.com/Tony-Y/pytorch_warmup)是一个用于PyTorch的学习率调度库。它的主要作用是通过在训练的早期阶段逐渐增加学习率，帮助模型更稳定地收敛。这种技术被称为\"学习率预热\"（learning rate warmup）。\n",
    "\n",
    "**1.作用：**\n",
    "- 稳定训练：在训练的初期，模型参数通常是随机初始化的，可能会导致梯度不稳定。通过使用较小的学习率，模型能够更稳定地更新参数，避免出现梯度爆炸或梯度消失的问题。\n",
    "- 加快收敛速度：在预热阶段之后，逐渐增加学习率可以帮助模型快速找到更优的参数，从而加快整体的收敛速度。\n",
    "- 提高模型性能：在某些情况下，特别是使用大规模数据和复杂模型时，学习率预热可以提高最终模型的性能\n",
    "\n",
    "**2. base和LinearWarmup源代码：**\n",
    "```python\n",
    "import math\n",
    "from contextlib import contextmanager\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "def _check_optimizer(optimizer):\n",
    "    if not isinstance(optimizer, Optimizer):\n",
    "        raise TypeError('{} ({}) is not an Optimizer.'.format(\n",
    "            optimizer, type(optimizer).__name__))\n",
    "\n",
    "\n",
    "class BaseWarmup(object):\n",
    "    \"\"\"Base class for all warmup schedules\n",
    "\n",
    "    Arguments:\n",
    "        optimizer (Optimizer): an instance of a subclass of Optimizer\n",
    "        warmup_params (list): warmup paramters\n",
    "        last_step (int): The index of last step. (Default: -1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, warmup_params, last_step=-1):\n",
    "        ......\n",
    "\n",
    "    def dampen(self, step=None):\n",
    "        \"\"\"Dampen the learning rates.\n",
    "\n",
    "        Arguments:\n",
    "            step (int): The index of current step. (Default: None)\n",
    "        \"\"\"\n",
    "        if step is None:\n",
    "            step = self.last_step + 1\n",
    "        self.last_step = step\n",
    "\n",
    "        for group, params in zip(self.optimizer.param_groups, self.warmup_params):\n",
    "            omega = self.warmup_factor(step, **params)\n",
    "            group['lr'] *= omega\n",
    "\n",
    "    @contextmanager\n",
    "    def dampening(self):\n",
    "        for group, lr in zip(self.optimizer.param_groups, self.lrs):\n",
    "            group['lr'] = lr\n",
    "        yield\n",
    "        self.lrs = [group['lr'] for group in self.optimizer.param_groups]\n",
    "        self.dampen()\n",
    "\n",
    "    def warmup_factor(self, step, **params):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def get_warmup_params(warmup_period, group_count):\n",
    "    ......\n",
    "\n",
    "class LinearWarmup(BaseWarmup):\n",
    "    \"\"\"Linear warmup schedule.\n",
    "\n",
    "    Arguments:\n",
    "        optimizer (Optimizer): an instance of a subclass of Optimizer\n",
    "        warmup_period (int or list): Warmup period\n",
    "        last_step (int): The index of last step. (Default: -1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, warmup_period, last_step=-1):\n",
    "        _check_optimizer(optimizer)\n",
    "        group_count = len(optimizer.param_groups)\n",
    "        warmup_params = get_warmup_params(warmup_period, group_count)\n",
    "        super(LinearWarmup, self).__init__(optimizer, warmup_params, last_step)\n",
    "\n",
    "    def warmup_factor(self, step, warmup_period):\n",
    "        return min(1.0, (step+1) / warmup_period)\n",
    "......\n",
    "```\n",
    "**3.使用方法之一：在预热完成后执行原有的lr_scheduler和执行逻辑**\n",
    "\n",
    "示例：\n",
    "\n",
    "```python\n",
    "warmup_period = 2000\n",
    "num_steps = len(dataloader) * num_epochs - warmup_period\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# 定义lr_schedduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\n",
    "# 定义warm_scheduler\n",
    "warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period)\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    for batch in dataloader:\n",
    "        ...\n",
    "        optimizer.step()\n",
    "        with warmup_scheduler.dampening():\n",
    "            if warmup_scheduler.last_step + 1 >= warmup_period:\n",
    "                lr_scheduler.step()\n",
    "```\n",
    "通过源代码分析，可知整体的执行逻辑：\n",
    "1. 进入上下文管理器：\n",
    "    - 通过`self.lrs = [group['lr'] for group in self.optimizer.param_groups]`，使得学习率恢复到初始值 0.01。\n",
    "2. 在上下文管理器中：\n",
    "    - 判断是否预热完成，完成执行lr_scheduler.step() 恢复原有的学习率调整策略。\n",
    "3. 退出上下文管理器：\n",
    "    - warmup_scheduler.dampen() 调整学习率，假设当前步数为 1000，新的学习率为 0.01 * 1000/2000 = 0.005。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb2c22",
   "metadata": {},
   "source": [
    "## 三、保存和加载模型\n",
    "\n",
    "在模型训练完成后，我们往往需要保存模型，毕竟是花费很多时间和精力才得到这个结果。\n",
    "\n",
    "### 3.1模型保存和加载\n",
    "模型保存有两种方式\n",
    "\n",
    "**1.保存参数（推荐）**\n",
    "\n",
    "仅保存模型的参数（state_dict），不包括模型的结构。适用于大多数情况下，因为模型的结构通常是已知的或容易重建的。\n",
    "使用`torch.save()`进行保存，需要输入要保存的模型参数和路径两个参数：\n",
    "```python\n",
    "# 初始化模型\n",
    "model = SimpleModel()\n",
    "\n",
    "# 保存模型的参数\n",
    "torch.save(model.state_dict(), 'model_params.pth')\n",
    "```\n",
    "对应的加载方式如下：\n",
    "```python\n",
    "# 初始化模型\n",
    "model = SimpleModel()\n",
    "\n",
    "# 加载模型的参数\n",
    "model.load_state_dict(torch.load('model_params.pth'))\n",
    "```\n",
    "\n",
    "**2.保存整个模型**\n",
    "\n",
    "将保存模型的结构和参数，但在加载时可能会有一些不便，因为它依赖于 PyTorch 的版本和环境。\n",
    "```python\n",
    "# 保存整个模型\n",
    "torch.save(model, 'model_complete.pth')\n",
    "\n",
    "# 加载整个模型\n",
    "model = torch.load('model_complete.pth')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5009ab",
   "metadata": {},
   "source": [
    "### 3.2 中断训练再继续\n",
    "在长时间或计算密集型任务（例如大规模数据处理、复杂模拟、机器学习训练等）中，通过定期保存程序状态（即“检查点”），以便在发生中断或失败时能够从最近的检查点恢复继续执行任务，而不必从头开始。\n",
    "\n",
    "检查点checkpoint通常需要保存`epoch`、模型参数`model.state_dict()`、优化器参数`optimizer.state_dict()`、`loss`\n",
    "\n",
    "以下是常用的一个模版：\n",
    "\n",
    "保存\n",
    "```python\n",
    "checkpoint_path = \"checkpoint.pth\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ......\n",
    "    \n",
    "    # 保存检查点\n",
    "    if (epoch + 1) % 5 == 0:  # 每5个epoch保存一次\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item(),\n",
    "        }, checkpoint_path)\n",
    "```\n",
    "加载\n",
    "```python\n",
    "# 恢复检查点\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "\n",
    "# 继续训练\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    ......\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc0aa7a",
   "metadata": {},
   "source": [
    "## 四、一整个pytorch训练流程\n",
    "\n",
    "## 数据集\n",
    "\n",
    "在本教程中，我们将使用由 TorchVision 提供的 Fashion-MNIST 数据集。我们将应用 `torchvision.transforms.Normalize()` 来对图像像素分布进行零均值化和标准化处理，并下载训练和验证数据子集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ef9a098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共有15000个批次\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_warmup as warmup\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# 创建训练集和验证集Dataset，采用直接加载已有数据方式\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# 创建训练集和验证集Dataloder\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# 创建TensorBoard供可视化使用\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "\n",
    "print('一共有{0}个批次'.format(len(training_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c9c8e",
   "metadata": {},
   "source": [
    "## 模型\n",
    "\n",
    "在本例中使用的模型是LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c1558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        output = self.fc3(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "model = GarmentClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888988f5",
   "metadata": {},
   "source": [
    "## 训练优化\n",
    "定义损失函数，定义优化器，进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2945e3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0:avg_loss is 1.3268629458775745 and avg_vloss is 0.7421066809244454\n",
      "EPOCH 1:avg_loss is 0.6260463508836615 and avg_vloss is 0.5558972305336967\n",
      "EPOCH 2:avg_loss is 0.4924594782317057 and avg_vloss is 0.4669824786595302\n",
      "EPOCH 3:avg_loss is 0.407467496813304 and avg_vloss is 0.4158334927924152\n",
      "EPOCH 4:avg_loss is 0.35879518366039653 and avg_vloss is 0.35718720184658304\n",
      "EPOCH 5:avg_loss is 0.3295985228778266 and avg_vloss is 0.3449697941861348\n",
      "EPOCH 6:avg_loss is 0.31016796532674346 and avg_vloss is 0.32182825101478085\n",
      "EPOCH 7:avg_loss is 0.2941968655638056 and avg_vloss is 0.33906078571940745\n",
      "EPOCH 8:avg_loss is 0.28001735381654813 and avg_vloss is 0.33215123484307313\n",
      "EPOCH 9:avg_loss is 0.2694252235427812 and avg_vloss is 0.3139313252239832\n",
      "EPOCH 10:avg_loss is 0.25494758353750957 and avg_vloss is 0.29832589491984907\n",
      "EPOCH 11:avg_loss is 0.24229037467952394 and avg_vloss is 0.30898909759426424\n",
      "EPOCH 12:avg_loss is 0.23195806347755116 and avg_vloss is 0.2883595319808872\n",
      "EPOCH 13:avg_loss is 0.22350500436567652 and avg_vloss is 0.30961378327836103\n",
      "EPOCH 14:avg_loss is 0.21321670605565288 and avg_vloss is 0.3215613886469478\n",
      "EPOCH 15:avg_loss is 0.20651429675989472 and avg_vloss is 0.310877525582275\n",
      "EPOCH 16:avg_loss is 0.19981959760266893 and avg_vloss is 0.31633573905144885\n",
      "EPOCH 17:avg_loss is 0.19383119266012455 and avg_vloss is 0.3039232044512681\n",
      "EPOCH 18:avg_loss is 0.187643148446315 and avg_vloss is 0.3250981241693399\n",
      "EPOCH 19:avg_loss is 0.18017318020456138 and avg_vloss is 0.32489270135462095\n",
      "EPOCH 20:avg_loss is 0.17746195070347306 and avg_vloss is 0.3593046113070705\n",
      "EPOCH 21:avg_loss is 0.17414706089914114 and avg_vloss is 0.33795956474774846\n",
      "EPOCH 22:avg_loss is 0.16769721360401116 and avg_vloss is 0.3361371440297415\n",
      "EPOCH 23:avg_loss is 0.16180437634044065 and avg_vloss is 0.37317945248528145\n",
      "EPOCH 24:avg_loss is 0.10972637116103412 and avg_vloss is 0.331890901968178\n",
      "EPOCH 25:avg_loss is 0.09504656291056932 and avg_vloss is 0.338763972245053\n",
      "EPOCH 26:avg_loss is 0.08964981890620818 and avg_vloss is 0.34815424578800397\n",
      "EPOCH 27:avg_loss is 0.08568534743302948 and avg_vloss is 0.3544933723692913\n",
      "EPOCH 28:avg_loss is 0.08235235248765428 and avg_vloss is 0.3590112968093725\n",
      "EPOCH 29:avg_loss is 0.07917341368907092 and avg_vloss is 0.36219972312753634\n",
      "EPOCH 30:avg_loss is 0.07665907461209578 and avg_vloss is 0.36613143619289973\n",
      "EPOCH 31:avg_loss is 0.07436886435689635 and avg_vloss is 0.3709016245128495\n",
      "EPOCH 32:avg_loss is 0.07191086372785396 and avg_vloss is 0.3801572048500088\n",
      "EPOCH 33:avg_loss is 0.06956383835125633 and avg_vloss is 0.38474864068317755\n",
      "EPOCH 34:avg_loss is 0.06761301801509742 and avg_vloss is 0.38925759167747737\n",
      "EPOCH 35:avg_loss is 0.06133420685250464 and avg_vloss is 0.3909966034787584\n",
      "EPOCH 36:avg_loss is 0.06053661755450429 and avg_vloss is 0.39227450775092204\n",
      "EPOCH 37:avg_loss is 0.06016393958538548 and avg_vloss is 0.39377298133076827\n",
      "EPOCH 38:avg_loss is 0.059795124617010455 and avg_vloss is 0.39452825701806865\n",
      "EPOCH 39:avg_loss is 0.05957703888469893 and avg_vloss is 0.39611125737382047\n",
      "EPOCH 40:avg_loss is 0.0592897870495098 and avg_vloss is 0.39729867529886004\n",
      "EPOCH 41:avg_loss is 0.0590585529816415 and avg_vloss is 0.39807662937705024\n",
      "EPOCH 42:avg_loss is 0.0587875221797508 and avg_vloss is 0.39897924452207695\n",
      "EPOCH 43:avg_loss is 0.058599637243420405 and avg_vloss is 0.40062167784577624\n",
      "EPOCH 44:avg_loss is 0.05836802245323723 and avg_vloss is 0.40091414037742373\n",
      "EPOCH 45:avg_loss is 0.058150856927958613 and avg_vloss is 0.4019069224074684\n",
      "EPOCH 46:avg_loss is 0.0573785774900558 and avg_vloss is 0.4019167286522134\n",
      "EPOCH 47:avg_loss is 0.05733679850096873 and avg_vloss is 0.40190507352135524\n",
      "EPOCH 48:avg_loss is 0.05731395050561934 and avg_vloss is 0.4019791318373576\n",
      "EPOCH 49:avg_loss is 0.057286432076062904 and avg_vloss is 0.40205423850302047\n"
     ]
    }
   ],
   "source": [
    "# 损失函数，采用交叉熵损失\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 优化器、学习率调整和预热\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, )\n",
    "warmup_scheduler = warmup.LinearWarmup(optimizer, 10)\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_tloss = 0.0\n",
    "    running_vloss = 0.0\n",
    "    \n",
    "    for i, vdata in enumerate(training_loader):\n",
    "        tinputs, tlabels = vdata\n",
    "        toutputs = model(tinputs)\n",
    "        tloss = loss_fn(toutputs, tlabels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        tloss.backward()\n",
    "        optimizer.step()\n",
    "        running_tloss += tloss.item()\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss.item()   \n",
    "           \n",
    "    avg_loss = running_tloss / (len(training_loader))\n",
    "    avg_vloss = running_vloss / (len(validation_loader))\n",
    "\n",
    "    with warmup_scheduler.dampening():\n",
    "        if warmup_scheduler.last_step + 1 >= 9:\n",
    "            lr_scheduler.step(avg_vloss)\n",
    "    \n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch + 1)\n",
    "    writer.flush() \n",
    "    print('EPOCH {0}:avg_loss is {1} and avg_vloss is {2}'.format(epoch, avg_loss, avg_vloss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4ccb1",
   "metadata": {},
   "source": [
    "## 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_params.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter] *",
   "language": "python",
   "name": "conda-env-jupyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
