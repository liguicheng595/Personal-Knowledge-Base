{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06084dc4",
   "metadata": {},
   "source": [
    "# 在Pytorch中构建模型\n",
    "\n",
    "## 一.  `torch.nn.Module` and `torch.nn.Parameter`\n",
    "\n",
    "下面介绍下Pytorch中构建深度学习神经网络模型的工具。\n",
    "\n",
    " 除了 `Parameter` 类，本视频中讨论的所有类都是 `torch.nn.Module` 的子类。这是一个 PyTorch 的基类，旨在封装特定于 PyTorch 模型及其组件的行为。\n",
    "\n",
    "`torch.nn.Module` 的一个重要行为是注册参数。如果某个 `Module` 子类有学习权重，这些权重将以 `torch.nn.Parameter` 的实例表示。`Parameter` 类是 `torch.Tensor` 的一个子类，具有特殊的行为，即当它们被分配为 `Module` 的属性时，它们会被添加到该模块的参数列表中。这些参数可以通过 `Module` 类的 `parameters()` 方法访问。\n",
    "\n",
    "作为一个简单的例子，这里有一个非常简单的模型，包含两个线性层和一个激活函数。我们将创建它的一个实例，并让它报告其参数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1781db76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Information:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "Just one Layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "Model Parameters\n",
      "Parameter containing:\n",
      "tensor([[ 0.0347,  0.0155, -0.0610,  ..., -0.0927,  0.0529,  0.0664],\n",
      "        [-0.0722, -0.0812,  0.0077,  ...,  0.0350,  0.0907, -0.0966],\n",
      "        [-0.0651,  0.0902,  0.0804,  ..., -0.0492,  0.0146, -0.0066],\n",
      "        ...,\n",
      "        [ 0.0365,  0.0983,  0.0533,  ...,  0.0116, -0.0039,  0.0182],\n",
      "        [-0.0544, -0.0764, -0.0781,  ..., -0.0437, -0.0065, -0.0290],\n",
      "        [-0.0277,  0.0544,  0.0426,  ...,  0.0315,  0.0461,  0.0726]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0668, -0.0450,  0.0211,  0.0540, -0.0978,  0.0893, -0.0048,  0.0342,\n",
      "        -0.0672, -0.0296,  0.0169, -0.0863, -0.0832, -0.0590,  0.0322,  0.0914,\n",
      "        -0.0702, -0.0556,  0.0312,  0.0188, -0.0431,  0.0738,  0.0909, -0.0757,\n",
      "         0.0738, -0.0580, -0.0789,  0.0914,  0.0944,  0.0994, -0.0587, -0.0242,\n",
      "         0.0798,  0.0611, -0.0040,  0.0703, -0.0236, -0.0755,  0.0405, -0.0854,\n",
      "        -0.0123, -0.0124,  0.0301, -0.0861, -0.0879, -0.0984,  0.0081,  0.0361,\n",
      "        -0.0146, -0.0544, -0.0630,  0.0340, -0.0695,  0.0826, -0.0785, -0.0703,\n",
      "         0.0280,  0.0269,  0.0966,  0.0772, -0.0720,  0.0311, -0.0396, -0.0673,\n",
      "        -0.0718, -0.0135, -0.0217, -0.0483,  0.0680, -0.0036,  0.0290, -0.0106,\n",
      "         0.0832,  0.0336,  0.0803, -0.0272, -0.0293, -0.0403,  0.0828, -0.0902,\n",
      "        -0.0022,  0.0352,  0.0997,  0.0950,  0.0174,  0.0335, -0.0993,  0.0106,\n",
      "        -0.0202,  0.0066, -0.0610, -0.0640,  0.0952,  0.0228,  0.0731, -0.0222,\n",
      "        -0.0584,  0.0662, -0.0435,  0.0230, -0.0296, -0.0220, -0.0447, -0.0337,\n",
      "         0.0941,  0.0478, -0.0934, -0.0082, -0.0192, -0.0069,  0.0571,  0.0302,\n",
      "         0.0876,  0.0200,  0.0866, -0.0674,  0.0095,  0.0569, -0.0747, -0.0383,\n",
      "        -0.0844, -0.0703,  0.0653, -0.0346, -0.0243,  0.0119,  0.0307,  0.0663,\n",
      "        -0.0413, -0.0047,  0.0374,  0.0259,  0.0143,  0.0606, -0.0466,  0.0520,\n",
      "         0.0680,  0.0038, -0.0550, -0.0228, -0.0130, -0.0843, -0.0921,  0.0135,\n",
      "         0.0894, -0.0085, -0.0671,  0.0518, -0.0842,  0.0681, -0.0742, -0.0180,\n",
      "        -0.0476,  0.0842,  0.0464,  0.0465,  0.0367, -0.0219, -0.0730,  0.0912,\n",
      "         0.0156,  0.0318,  0.0191,  0.0477, -0.0082, -0.0625,  0.0683, -0.0581,\n",
      "         0.0663,  0.0334, -0.0833, -0.0699, -0.0586, -0.0636,  0.0384,  0.0348,\n",
      "         0.0024, -0.0388, -0.0993,  0.0256,  0.0677, -0.0064, -0.0535, -0.0912,\n",
      "         0.0452, -0.0334, -0.0286,  0.0313,  0.0047, -0.0659, -0.0785,  0.0103,\n",
      "         0.0089,  0.0965, -0.0481, -0.0713,  0.0429, -0.0228,  0.0018,  0.0427],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0388, -0.0426,  0.0528,  ..., -0.0132,  0.0264,  0.0098],\n",
      "        [-0.0457, -0.0018,  0.0013,  ...,  0.0335, -0.0322, -0.0073],\n",
      "        [ 0.0161, -0.0006, -0.0359,  ..., -0.0245,  0.0247, -0.0299],\n",
      "        ...,\n",
      "        [ 0.0240,  0.0380, -0.0240,  ...,  0.0464, -0.0453,  0.0049],\n",
      "        [-0.0433,  0.0542,  0.0445,  ...,  0.0502, -0.0036,  0.0605],\n",
      "        [ 0.0276, -0.0446,  0.0562,  ..., -0.0146, -0.0004,  0.0409]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0512, -0.0325,  0.0045, -0.0143,  0.0400,  0.0187, -0.0083,  0.0328,\n",
      "        -0.0128, -0.0381], requires_grad=True)\n",
      "\n",
      "Layer Parameters\n",
      "Parameter containing:\n",
      "tensor([[-0.0388, -0.0426,  0.0528,  ..., -0.0132,  0.0264,  0.0098],\n",
      "        [-0.0457, -0.0018,  0.0013,  ...,  0.0335, -0.0322, -0.0073],\n",
      "        [ 0.0161, -0.0006, -0.0359,  ..., -0.0245,  0.0247, -0.0299],\n",
      "        ...,\n",
      "        [ 0.0240,  0.0380, -0.0240,  ...,  0.0464, -0.0453,  0.0049],\n",
      "        [-0.0433,  0.0542,  0.0445,  ...,  0.0502, -0.0036,  0.0605],\n",
      "        [ 0.0276, -0.0446,  0.0562,  ..., -0.0146, -0.0004,  0.0409]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0512, -0.0325,  0.0045, -0.0143,  0.0400,  0.0187, -0.0083,  0.0328,\n",
      "        -0.0128, -0.0381], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('Model Information:')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\nJust One Layer:')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\nModel Parameters')\n",
    "for para in tinymodel.parameters():\n",
    "    print(para)\n",
    "    \n",
    "print('\\nLayer Parameters')\n",
    "for para in tinymodel.linear2.parameters():\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf18d8a6",
   "metadata": {},
   "source": [
    "这展示了 PyTorch 模型的基本结构：有一个 `__init__()` 方法用于定义模型的层和其他组件，以及一个 `forward()` 方法用于执行计算。需要注意的是，我们可以打印模型或其任何子模块，以了解其结构。\n",
    "\n",
    "## 二. 常见的层类型\n",
    "\n",
    "### 2.1 线性层\n",
    "\n",
    "神经网络中最基本的层类型是 *线性* 或 *全连接* 层。这种层中，每个输入对层的每个输出都有影响，影响程度由层的权重指定。如果模型有 *m* 个输入和 *n* 个输出，那么权重将是一个 *m × n* 的矩阵。例如：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d26b052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[0.8060, 0.1068, 0.1357]])\n",
      "\n",
      "\n",
      "Weight and Bias parameters:\n",
      "Parameter containing:\n",
      "tensor([[-0.4783,  0.1633,  0.1634],\n",
      "        [ 0.2393,  0.4522, -0.5212]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.5287, -0.4384], requires_grad=True)\n",
      "\n",
      "\n",
      "Weight\n",
      "Parameter containing:\n",
      "tensor([[-0.4783,  0.1633,  0.1634],\n",
      "        [ 0.2393,  0.4522, -0.5212]], requires_grad=True)\n",
      "\n",
      "\n",
      "Bias\n",
      "Parameter containing:\n",
      "tensor([ 0.5287, -0.4384], requires_grad=True)\n",
      "\n",
      "\n",
      "Output:\n",
      "tensor([[ 0.1829, -0.2680]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = torch.nn.Linear(3, 2)\n",
    "x = torch.rand(1, 3)\n",
    "print('Input:')\n",
    "print(x)\n",
    "\n",
    "print('\\n\\nWeight and Bias parameters:')\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "    \n",
    "print('\\n\\nWeight:')\n",
    "print(lin.weight)\n",
    "\n",
    "print('\\n\\nBias:')\n",
    "print(lin.bias)\n",
    "\n",
    "y = lin(x)\n",
    "print('\\n\\nOutput:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeffe17",
   "metadata": {},
   "source": [
    "如果对 `x` 进行线性层权重的矩阵乘法，并加上偏置，你将得到输出向量 `y`。\n",
    "\n",
    "还有一个重要的特性需要注意：当我们通过 `lin.weight` 检查层的权重（参数）时，它会报告自己是一个 `Parameter`（`Parameter` 是 `Tensor` 的子类），并告诉我们它正在使用自动求导跟踪梯度。这是 `Parameter` 的默认行为，与 `Tensor` 不同。\n",
    "\n",
    "**线性层在深度学习模型中被广泛使用。你通常会在分类器模型中看到它们的身影，通常位于末尾的一个或多个线性层会有 *n* 个输出，其中 *n* 是分类器要处理的类别数。**\n",
    "\n",
    "### 2.2 卷积层\n",
    "\n",
    "*卷积* 层专门处理具有高度空间相关性的数据。它们在计算机视觉中非常常见，用于检测特征的紧密组合，并将其组合成更高级别的特征。它们也出现在其他情境中，例如在自然语言处理应用中，一个单词的上下文（即序列中附近的其他单词）可以影响句子的含义。\n",
    "\n",
    "我们在早期的视频中看到了 LeNet5 中卷积层的应用：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "453aea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb17d0",
   "metadata": {},
   "source": [
    "让我们详细分析一下这个模型中卷积层的工作原理。首先从 `conv1` 开始：\n",
    "\n",
    "* LeNet5 设计用于接收 1x32x32 的黑白图像。**卷积层构造函数的第一个参数是输入通道的数量。** 这里是 1。如果我们要构建一个处理三色通道的模型，这个值将是 3。\n",
    "* 卷积层类似于一个在图像上扫描的窗口，寻找它能识别的模式。这些模式称为*特征*，卷积层的一个参数是我们希望它学习的特征数量。**构造函数的第二个参数是输出特征的数量。** 在这里，我们要求我们的层学习 6 个特征。\n",
    "* 我刚才把卷积层比作一个窗口 - 但这个窗口有多大呢？**第三个参数是窗口或*核*的大小。** 这里的“5”意味着我们选择了一个 5x5 的核。（如果你想要一个高度不同于宽度的核，可以为此参数指定一个元组 - 例如，`(3, 5)` 表示一个 3x5 的卷积核。）\n",
    "\n",
    "卷积层的输出是一个*激活图* - 输入张量中特征存在的空间表示。`conv1` 将给我们一个 6x28x28 的输出张量；6 是特征的数量，28 是激活图的高度和宽度。（28 来自于当在 32 像素行上扫描一个 5 像素窗口时，只有 28 个有效位置。）\n",
    "\n",
    "然后我们将卷积层的输出通过一个 ReLU 激活函数（稍后会详细介绍激活函数），然后通过一个最大池化层。最大池化层将激活图中彼此相邻的特征进行分组。它通过减少张量，将输出中的每个 2x2 组单元合并为一个单元，并将该单元的值设为进入它的 4 个单元中的最大值。这给了我们一个低分辨率的激活图，维度为 6x14x14。\n",
    "\n",
    "我们的下一个卷积层 `conv2` 期望 6 个输入通道（对应于第一层中寻找的 6 个特征），有 16 个输出通道，并且有一个 3x3 的核。它输出一个 16x12x12 的激活图，再次通过一个最大池化层缩小为 16x6x6。在将此输出传递给线性层之前，它被重塑为一个 16 * 6 * 6 = 576 元素的向量，以供下一层使用。\n",
    "\n",
    "有针对 1D、2D 和 3D 张量的卷积层。卷积层构造函数还有许多其他可选参数，包括步长（例如，只扫描每隔一个或每隔两个位置）、填充（这样你可以扫描到输入的边缘）等。请参阅 [文档](https://pytorch.org/docs/stable/nn.html#convolution-layers) 了解更多信息。\n",
    "\n",
    "![卷积层示意图](https://www.baeldung.com/wp-content/uploads/sites/4/2021/10/conv_pooling-2048x677.png)\n",
    "\n",
    "\n",
    "### 2.3 循环层\n",
    "\n",
    "*循环神经网络*（或 *RNNs*）用于处理序列数据——从科学仪器的时间序列测量到自然语言句子再到 DNA 核苷酸。RNN 通过维护一个*隐藏状态*来实现，这个隐藏状态相当于对序列中迄今为止所见内容的一种记忆。\n",
    "\n",
    "RNN 层的内部结构——或其变体 LSTM（长短期记忆）和 GRU（门控循环单元）——相对复杂，超出了本视频的范围，但我们将向你展示一个基于 LSTM 的词性标注器的实际效果（这是一种分类器，用于告诉你一个词是名词、动词等）：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e79db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores      \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469750e",
   "metadata": {},
   "source": [
    "构造函数有四个参数：\n",
    "\n",
    "* `vocab_size` 是输入词汇表中的单词数量。每个单词在一个 `vocab_size` 维空间中表示为一个独热向量（或单位向量）。\n",
    "* `tagset_size` 是输出标签集中的标签数量。\n",
    "* `embedding_dim` 是词汇表的*嵌入*空间的大小。嵌入将词汇映射到一个低维空间，在这个空间中，具有相似含义的单词彼此接近。\n",
    "* `hidden_dim` 是 LSTM 的记忆单元的大小。\n",
    "\n",
    "输入将是一个句子，单词用独热向量的索引表示。嵌入层会将这些索引映射到一个 `embedding_dim` 维的空间。LSTM 接受这个嵌入序列并迭代处理，生成一个长度为 `hidden_dim` 的输出向量。最终的线性层充当分类器；对最终层的输出应用 `log_softmax()` 将输出转换为一组归一化的概率估计，这些概率表示给定单词对应给定标签的可能性。\n",
    "\n",
    "如果你想看到这个网络的实际运行效果，请查看 pytorch.org 上的[序列模型和 LSTM 网络](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)教程。\n",
    "\n",
    "![LSTM](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*laH0_xXEkFE0lKJu54gkFQ.png)\n",
    "\n",
    "\n",
    "### 2.3 Transformers\n",
    "\n",
    "*Transformers* 是多功能网络，已通过像 BERT 这样的模型在 NLP 中占据了最前沿的地位。对 transformer 架构的讨论超出了本视频的范围，但 PyTorch 有一个 `Transformer` 类，可以让你定义 transformer 模型的总体参数——注意力头的数量、编码器和解码器层的数量、dropout 和激活函数等。（你甚至可以从这个单类中构建 BERT 模型，只需设置正确的参数！）`torch.nn.Transformer` 类还有一些类，用于封装各个组件（`TransformerEncoder`、`TransformerDecoder`）和子组件（`TransformerEncoderLayer`、`TransformerDecoderLayer`）。有关详细信息，请查看 [transformer 类的文档](https://pytorch.org/docs/stable/nn.html#transformer)，以及 pytorch.org 上的相关[教程](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)。\n",
    "\n",
    "[YouTube关于transformer的教程](https://youtu.be/eMlx5fFNoYc?si=RLzkSjKRMP1EjvLs)\n",
    "\n",
    "[Transformer模型详解（图解最完整版）知乎](https://zhuanlan.zhihu.com/p/338817680)\n",
    "\n",
    "## 三. 其他层和函数\n",
    "\n",
    "### 3.1 数据处理层\n",
    "\n",
    "还有一些其他类型的层在模型中执行重要功能，但**它们本身并不参与学习过程。**\n",
    "\n",
    "**池化层**通过组合单元来减少张量，并按着某一个规则（最大化、最小化、平均等）配给输出单元。\n",
    "![池化层](https://paddlepedia.readthedocs.io/en/latest/_images/avgpooling_maxpooling.png)\n",
    "例如：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a15aa20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8179, 0.4223, 0.1031, 0.4998, 0.9807, 0.7883],\n",
      "         [0.0383, 0.0148, 0.1133, 0.5124, 0.4422, 0.5419],\n",
      "         [0.7136, 0.3201, 0.8491, 0.3267, 0.7967, 0.4837],\n",
      "         [0.5383, 0.3686, 0.9351, 0.9668, 0.3571, 0.8391],\n",
      "         [0.3417, 0.4488, 0.4175, 0.0563, 0.9025, 0.5393],\n",
      "         [0.2174, 0.7326, 0.4704, 0.1715, 0.0172, 0.0554]]])\n",
      "tensor([[[0.8491, 0.9807],\n",
      "         [0.9351, 0.9668]]])\n",
      "tensor([[[0.3769, 0.5969],\n",
      "         [0.4967, 0.4339]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 6, 6)\n",
    "print(my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(3)\n",
    "avgpool_layer = torch.nn.AvgPool2d(3)\n",
    "print(maxpool_layer(my_tensor))\n",
    "print(avgpool_layer(my_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e65ec",
   "metadata": {},
   "source": [
    "从结果能够看到，如果使用MaxPool2d，则获得的值是窗口内最大值，如果是AvgPool2d则是窗口内的平均值\n",
    "\n",
    "**归一化层** 在将一个层的输出传递到另一个层之前，对其进行重新中心化和归一化。对中间张量进行中心化和缩放有许多有益的效果，例如允许你使用更高的学习率而**不会导致梯度爆炸或消失**。\n",
    "[常见的归一化层资料](https://siyuanblog.cn/archives/normalization#toc-head-2)\n",
    "\n",
    "在PyTorch中，有几种常用的归一化层（Normalization Layers），主要包括以下几种：\n",
    "\n",
    "1. **Batch Normalization (批归一化)**:\n",
    "   - `torch.nn.BatchNorm1d`: 适用于1D输入，比如序列数据。\n",
    "   - `torch.nn.BatchNorm2d`: 适用于2D输入，比如图像数据。\n",
    "   - `torch.nn.BatchNorm3d`: 适用于3D输入，比如视频数据。\n",
    "\n",
    "2. **Layer Normalization (层归一化)**:\n",
    "   - `torch.nn.LayerNorm`: 可以用于任意维度的输入，通过指定归一化维度来实现。\n",
    "\n",
    "3. **Weight Normalization (权重归一化)**:\n",
    "   - `torch.nn.utils.weight_norm`: 通过对权重进行归一化来加速训练和提高稳定性。\n",
    "\n",
    "4. Instance Normalization (实例归一化):\n",
    "   - `torch.nn.InstanceNorm1d`: 适用于1D输入。\n",
    "   - `torch.nn.InstanceNorm2d`: 适用于2D输入。\n",
    "   - `torch.nn.InstanceNorm3d`: 适用于3D输入。\n",
    "\n",
    "5. Group Normalization (组归一化):\n",
    "   - `torch.nn.GroupNorm`: 适用于任意维度的输入，通过指定组数进行归一化。\n",
    "\n",
    "6. Local Response Normalization (局部响应归一化):\n",
    "   - `torch.nn.LocalResponseNorm`: 一种较早使用的归一化方法，主要用于某些特定的网络结构。\n",
    "\n",
    "\n",
    "这些归一化层在训练神经网络时可以帮助加速收敛、提高训练稳定性和防止过拟合。根据具体任务的需求和数据特点，可以选择合适的归一化层来应用。\n",
    "下面是一些实例：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9307d865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BatchNorm model\n",
      "Epoch [10/100], Loss: 1.3903\n",
      "Epoch [20/100], Loss: 1.2507\n",
      "Epoch [30/100], Loss: 1.1410\n",
      "Epoch [40/100], Loss: 1.0537\n",
      "Epoch [50/100], Loss: 0.9698\n",
      "Epoch [60/100], Loss: 0.8940\n",
      "Epoch [70/100], Loss: 0.8412\n",
      "Epoch [80/100], Loss: 0.7981\n",
      "Epoch [90/100], Loss: 0.7666\n",
      "Epoch [100/100], Loss: 0.7432\n",
      "\n",
      "Training LayerNorm model\n",
      "Epoch [10/100], Loss: 1.4467\n",
      "Epoch [20/100], Loss: 1.3105\n",
      "Epoch [30/100], Loss: 1.2118\n",
      "Epoch [40/100], Loss: 1.1394\n",
      "Epoch [50/100], Loss: 1.0827\n",
      "Epoch [60/100], Loss: 1.0310\n",
      "Epoch [70/100], Loss: 0.9883\n",
      "Epoch [80/100], Loss: 0.9480\n",
      "Epoch [90/100], Loss: 0.9177\n",
      "Epoch [100/100], Loss: 0.8892\n",
      "\n",
      "Training WeightNorm model\n",
      "Epoch [10/100], Loss: 0.9603\n",
      "Epoch [20/100], Loss: 0.9318\n",
      "Epoch [30/100], Loss: 0.9101\n",
      "Epoch [40/100], Loss: 0.8906\n",
      "Epoch [50/100], Loss: 0.8733\n",
      "Epoch [60/100], Loss: 0.8560\n",
      "Epoch [70/100], Loss: 0.8392\n",
      "Epoch [80/100], Loss: 0.8231\n",
      "Epoch [90/100], Loss: 0.8078\n",
      "Epoch [100/100], Loss: 0.7932\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "\n",
    "# 定义带有Batch Normalization的前馈神经网络\n",
    "class BatchNormNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BatchNormNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        return x\n",
    "\n",
    "# 定义带有Layer Normalization的前馈神经网络\n",
    "class LayerNormNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LayerNormNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.ln2 = nn.LayerNorm(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.ln1(self.fc1(x)))\n",
    "        x = F.relu(self.ln2(self.fc2(x)))\n",
    "        return x\n",
    "\n",
    "# 定义带有Weight Normalization的前馈神经网络\n",
    "class WeightNormNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(WeightNormNN, self).__init__()\n",
    "        self.fc1 = weight_norm(nn.Linear(input_dim, hidden_dim))\n",
    "        self.fc2 = weight_norm(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# 定义一个简单的训练函数\n",
    "def train(model, criterion, optimizer, data, target):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# 创建一些随机数据\n",
    "input_dim = 10\n",
    "hidden_dim = 20\n",
    "output_dim = 5\n",
    "batch_size = 32\n",
    "\n",
    "data = torch.randn(batch_size, input_dim)\n",
    "target = torch.randn(batch_size, output_dim)\n",
    "\n",
    "# 创建模型、损失函数和优化器\n",
    "models = {\n",
    "    'BatchNorm': BatchNormNN(input_dim, hidden_dim, output_dim),\n",
    "    'LayerNorm': LayerNormNN(input_dim, hidden_dim, output_dim),\n",
    "    'WeightNorm': WeightNormNN(input_dim, hidden_dim, output_dim)\n",
    "}\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizers = {\n",
    "    'BatchNorm': optim.Adam(models['BatchNorm'].parameters(), lr=0.001),\n",
    "    'LayerNorm': optim.Adam(models['LayerNorm'].parameters(), lr=0.001),\n",
    "    'WeightNorm': optim.Adam(models['WeightNorm'].parameters(), lr=0.001)\n",
    "}\n",
    "\n",
    "# 训练每个模型并输出损失\n",
    "num_epochs = 100\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model\")\n",
    "    optimizer = optimizers[name]\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train(model, criterion, optimizer, data, target)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796222e",
   "metadata": {},
   "source": [
    "**Dropout 层** 是一种促进模型中*稀疏表示*的工具——即推动模型使用更少的数据进行推理。\n",
    "\n",
    "Dropout 层的工作原理是在训练期间随机设置输入张量的部分为零——dropout 层在推理时总是关闭的。这迫使模型在这个掩码或缩减的数据集上学习。例如："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c261f7",
   "metadata": {},
   "source": [
    "上图中，你可以看到 dropout 对样本张量的影响。你可以使用可选参数 `p` 来设置个别权重被 dropout 的概率；如果不设置，默认值是 0.5。\n",
    "\n",
    "### 3.2 激活函数\n",
    "\n",
    "激活函数使深度学习成为可能。神经网络实际上是一个具有许多参数的程序，它*模拟一个数学函数*。如果我们只是重复地用层权重乘以张量，我们只能模拟*线性函数*；此外，拥有多层也没有意义，因为整个网络可以简化为一个矩阵乘法。插入*非线性*激活函数在层之间，使深度学习模型能够模拟任何函数，而不仅仅是线性函数。\n",
    "\n",
    "`torch.nn.Module` 包含封装所有主要激活函数的对象，包括 ReLU 及其多种变体、Tanh、Hardtanh、sigmoid 等。它还包括其他函数，如 Softmax，这些函数在模型的输出阶段最有用。\n",
    "\n",
    "### 3.3 损失函数\n",
    "\n",
    "损失函数告诉我们模型的预测与正确答案的差距。PyTorch 包含多种损失函数，包括常见的 MSE（均方误差 = L2 范数）、交叉熵损失和负似然损失（对分类器有用）等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef07b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter] *",
   "language": "python",
   "name": "conda-env-jupyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
